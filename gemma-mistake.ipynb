{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-21T07:36:38.674818Z","iopub.execute_input":"2024-07-21T07:36:38.675216Z","iopub.status.idle":"2024-07-21T07:36:39.638558Z","shell.execute_reply.started":"2024-07-21T07:36:38.675181Z","shell.execute_reply":"2024-07-21T07:36:39.637646Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## dataset creation","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset, Dataset\nimport random\nfrom tqdm.auto import tqdm\nimport os","metadata":{"execution":{"iopub.status.busy":"2024-07-21T07:36:39.640530Z","iopub.execute_input":"2024-07-21T07:36:39.641003Z","iopub.status.idle":"2024-07-21T07:36:40.956279Z","shell.execute_reply.started":"2024-07-21T07:36:39.640961Z","shell.execute_reply":"2024-07-21T07:36:40.955212Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"dataset = load_dataset(\"styalai/SlimPajama-1M-rows\", split=\"train[:1%]\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = {\"text\":[]}\nprobs = 0.2\na = list(\"azertyuiopmlkjhgfdsqwxcvbn,?:.1234567890AZERTYUIOPMLKJHGFDSQWXCVBN\")\n\nfor t in tqdm(dataset[\"text\"][:1000]):\n    text = \"<bos><start_of_turn>user\\n\"\n    s = list(t)[-1000:]\n    for l in range(len(s)):\n        if random.random() < probs and s[l] != ' ': # change the letter\n            s[l] = a[random.randint(0, len(a)-1)]\n    s = ''.join(s)\n    text = text + s + \"<end_of_turn>\\n<start_of_turn>model\\n\" + t[-1000:] + \"<end_of_turn>\"\n    data['text'].append(text)\n\ndata = pd.DataFrame.from_dict(data)\ndata = Dataset.from_pandas(data)\ndata.push_to_hub(\"styalai/mistake-v2\", token=\"hf_token\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LOAD DATASET","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset(\"styalai/mistake-v2\")","metadata":{"execution":{"iopub.status.busy":"2024-07-20T19:47:46.786752Z","iopub.execute_input":"2024-07-20T19:47:46.788104Z","iopub.status.idle":"2024-07-20T19:47:48.434453Z","shell.execute_reply.started":"2024-07-20T19:47:46.788060Z","shell.execute_reply":"2024-07-20T19:47:48.433577Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/274 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6e7b4e47cdf4ffca6e84d786c0d1e5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.25M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45ff2e537a4d4dcb9e52112dc6257c99"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1001 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e328eb832dee4ded9b69d4327de71dee"}},"metadata":{}}]},{"cell_type":"code","source":"dataset['train']","metadata":{"execution":{"iopub.status.busy":"2024-07-20T19:47:48.435993Z","iopub.execute_input":"2024-07-20T19:47:48.436307Z","iopub.status.idle":"2024-07-20T19:47:48.443105Z","shell.execute_reply.started":"2024-07-20T19:47:48.436281Z","shell.execute_reply":"2024-07-20T19:47:48.441973Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['text'],\n    num_rows: 1001\n})"},"metadata":{}}]},{"cell_type":"code","source":"data = pd.DataFrame(dataset['train'])\nos.mkdir(\"/kaggle/working/data\")\ndata.to_csv('data/dataset.csv', index=False, escapechar='\\\\')  ","metadata":{"execution":{"iopub.status.busy":"2024-07-20T19:48:01.361863Z","iopub.execute_input":"2024-07-20T19:48:01.362283Z","iopub.status.idle":"2024-07-20T19:48:01.488720Z","shell.execute_reply.started":"2024-07-20T19:48:01.362251Z","shell.execute_reply":"2024-07-20T19:48:01.487849Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## fine tune","metadata":{}},{"cell_type":"code","source":"import os\n!pip install -U autotrain-advanced \n!autotrain setup","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@markdown ---\n#@markdown #### Project Config\n#@markdown Note: if you are using a restricted/private model, you need to enter your Hugging Face token in the next step.\nproject_name = 'gemmamistakes-v3' # @param {type:\"string\"}\nmodel_name = \"styalai/gemmamistakes-v2\" # model to fine tune\n\n#@markdown ---\n#@markdown #### Push to Hub?\n#@markdown Use these only if you want to push your trained model to a private repo in your Hugging Face Account\n#@markdown If you dont use these, the model will be saved in Google Colab and you are required to download it manually.\n#@markdown Please enter your Hugging Face write token. The trained model will be saved to your Hugging Face account.\n#@markdown You can find your token here: https://huggingface.co/settings/tokens\npush_to_hub = True # @param [\"False\", \"True\"] {type:\"raw\"}\nhf_token = \"hf_token\" #@param {type:\"string\"}\n#repo_id = \"styalai/gemmamistakes-v1\" #@param {type:\"string\"}\n\n#@markdown ---\n#@markdown #### Hyperparameters\nlearning_rate = 3e-4 # @param {type:\"number\"}\nnum_epochs = 2 #@param {type:\"number\"}\nbatch_size = 1 # @param {type:\"slider\", min:1, max:32, step:1}\nblock_size = 1024 # @param {type:\"number\"}\ntrainer = \"sft\" # @param [\"default\", \"sft\"] {type:\"raw\"}\nwarmup_ratio = 0.1 # @param {type:\"number\"}\nweight_decay = 0.01 # @param {type:\"number\"}\ngradient_accumulation = 4 # @param {type:\"number\"}\nmixed_precision = \"fp16\" # @param [\"fp16\", \"bf16\", \"none\"] {type:\"raw\"}\npeft = True # @param [\"False\", \"True\"] {type:\"raw\"}\nquantization = \"int4\" # @param [\"int4\", \"int8\", \"none\"] {type:\"raw\"}\nlora_r = 16 #@param {type:\"number\"}\nlora_alpha = 32 #@param {type:\"number\"}\nlora_dropout = 0.05 #@param {type:\"number\"}\n\nos.environ[\"PROJECT_NAME\"] = project_name\nos.environ[\"MODEL_NAME\"] = model_name\nos.environ[\"PUSH_TO_HUB\"] = str(push_to_hub)\nos.environ[\"HF_TOKEN\"] = hf_token\n#os.environ[\"REPO_ID\"] = repo_id\nos.environ[\"LEARNING_RATE\"] = str(learning_rate)\nos.environ[\"NUM_EPOCHS\"] = str(num_epochs)\nos.environ[\"BATCH_SIZE\"] = str(batch_size)\nos.environ[\"BLOCK_SIZE\"] = str(block_size)\nos.environ[\"WARMUP_RATIO\"] = str(warmup_ratio)\nos.environ[\"WEIGHT_DECAY\"] = str(weight_decay)\nos.environ[\"GRADIENT_ACCUMULATION\"] = str(gradient_accumulation)\nos.environ[\"MIXED_PRECISION\"] = str(mixed_precision)\nos.environ[\"PEFT\"] = str(peft)\nos.environ[\"QUANTIZATION\"] = str(quantization)\nos.environ[\"LORA_R\"] = str(lora_r)\nos.environ[\"LORA_ALPHA\"] = str(lora_alpha)\nos.environ[\"LORA_DROPOUT\"] = str(lora_dropout)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T19:49:42.435862Z","iopub.execute_input":"2024-07-20T19:49:42.436236Z","iopub.status.idle":"2024-07-20T19:49:42.449220Z","shell.execute_reply.started":"2024-07-20T19:49:42.436207Z","shell.execute_reply":"2024-07-20T19:49:42.448082Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"!autotrain llm \\\n--train \\\n--username \"styalai\" \\\n--merge-adapter \\\n--model ${MODEL_NAME} \\\n--project-name ${PROJECT_NAME} \\\n--data-path data/ \\\n--text-column text \\\n--lr ${LEARNING_RATE} \\\n--batch-size ${BATCH_SIZE} \\\n--epochs ${NUM_EPOCHS} \\\n--block-size ${BLOCK_SIZE} \\\n--warmup-ratio ${WARMUP_RATIO} \\\n--lora-r ${LORA_R} \\\n--lora-alpha ${LORA_ALPHA} \\\n--lora-dropout ${LORA_DROPOUT} \\\n--weight-decay ${WEIGHT_DECAY} \\\n--gradient-accumulation ${GRADIENT_ACCUMULATION} \\\n--quantization ${QUANTIZATION} \\\n--mixed-precision ${MIXED_PRECISION} \\\n$( [[ \"$PEFT\" == \"True\" ]] && echo \"--peft\" ) \\\n$( [[ \"$PUSH_TO_HUB\" == \"True\" ]] && echo \"--push-to-hub --token ${HF_TOKEN}\" )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\nfrom transformers import TextStreamer\nimport torch\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2024-07-21T07:36:45.149561Z","iopub.execute_input":"2024-07-21T07:36:45.150125Z","iopub.status.idle":"2024-07-21T07:37:01.218969Z","shell.execute_reply.started":"2024-07-21T07:36:45.150091Z","shell.execute_reply":"2024-07-21T07:37:01.218058Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-07-21 07:36:51.454506: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-21 07:36:51.454617: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-21 07:36:51.564949: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"model_id = \"styalai/gemmamistakes-v2\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T07:37:01.220772Z","iopub.execute_input":"2024-07-21T07:37:01.221556Z","iopub.status.idle":"2024-07-21T07:37:32.937557Z","shell.execute_reply.started":"2024-07-21T07:37:01.221517Z","shell.execute_reply":"2024-07-21T07:37:32.936740Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/40.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b32020a0c7e6482290d25cc4a1295d2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4eaf64b0c52046cd8b008ec781b511bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78b61c08de394274a91d542c3930df7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26d392a5c7cb4fd18ddc51b015ba478a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/712 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8de79ceeeaa4368af55cb709246d4b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9df01d8e39a844658d64cbeae850c041"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e02cfd35b8c4efb8951fde308507a10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e72cd28fd1b648edb709b64e9847ec50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1eb1df74d194c1aad4c9b8ed164744e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3512e085950245d0a73ce1632c75779c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbbaa526c23d4e529bece6ac2a94e655"}},"metadata":{}}]},{"cell_type":"code","source":"chat = [\n    { \"role\": \"user\", \"content\": \"u?d meZtion diet t1 Fe, but it did lqttle to swry ,e. Whyr Bad examyles. nrap Pitt in Ocean's Eleven.\"},\n]\nquestion = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n\nquestion = tokenizer(question, return_tensors=\"pt\").to(device)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T07:47:29.490503Z","iopub.execute_input":"2024-07-21T07:47:29.490867Z","iopub.status.idle":"2024-07-21T07:47:29.497208Z","shell.execute_reply.started":"2024-07-21T07:47:29.490837Z","shell.execute_reply":"2024-07-21T07:47:29.496298Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"streamer = TextStreamer(tokenizer, skip_prompt=True)\n\n_ = model.generate(**question, streamer=streamer,\n                            eos_token_id=tokenizer.bos_token_id,\n                            max_length=2048, \n                            temperature=0,\n                            top_p=0.8,\n                            repetition_penalty=1.25)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T07:47:30.476250Z","iopub.execute_input":"2024-07-21T07:47:30.476641Z","iopub.status.idle":"2024-07-21T07:47:31.711284Z","shell.execute_reply.started":"2024-07-21T07:47:30.476610Z","shell.execute_reply":"2024-07-21T07:47:31.710325Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"You would mention diet to be a way to lose weight, but it did little to save me. Why? Because bad examples. In the movie Ocean's Eleven, Brad Pitt in Ocean's Eleven.<end_of_turn><bos>\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}